import os
import json
import re
import time
import logging
import hashlib
import html
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta, time as dt_time
from dateutil import parser
from zoneinfo import ZoneInfo
from typing import Dict, List, Optional, Tuple, Union
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from azure.data.tables import TableServiceClient
from openai import AzureOpenAI

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


class EmailProcessor:
    """ì´ë©”ì¼ ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        """ì´ˆê¸°í™” ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ"""

        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        self.azure_openai_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        self.azure_openai_key = os.getenv("AZURE_OPENAI_KEY")
        self.azure_openai_deployment_chat = os.getenv(
            "AZURE_OPENAI_DEPLOYMENT_CHAT", "gpt-4"
        )
        self.azure_openai_deployment_emb = os.getenv(
            "AZURE_OPENAI_DEPLOYMENT_EMBEDDING", "text-embedding-3-small"
        )
        self.ai_search_endpoint = os.getenv("AI_SEARCH_ENDPOINT")
        self.ai_search_index = os.getenv("AI_SEARCH_INDEX", "emails-index")
        self.ai_search_admin_key = os.getenv("AI_SEARCH_ADMIN_KEY")
        self.azure_storage_connection_string = os.getenv(
            "AZURE_STORAGE_CONNECTION_STRING"
        )
        self.default_confidence = float(os.getenv("DEFAULT_CONFIDENCE", "0.65"))

        # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
        self._validate_environment()

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._initialize_clients()

        # ì„ë² ë”© ë°°í¬ëª… ìë™ ê°ì§€
        self._detect_embedding_deployment()

    def _validate_environment(self):
        """í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦"""

        required_vars = {
            "AZURE_OPENAI_ENDPOINT": self.azure_openai_endpoint,
            "AZURE_OPENAI_KEY": self.azure_openai_key,
            "AI_SEARCH_ENDPOINT": self.ai_search_endpoint,
            "AI_SEARCH_ADMIN_KEY": self.ai_search_admin_key,
            "AZURE_STORAGE_CONNECTION_STRING": self.azure_storage_connection_string,
        }

        missing_vars = [key for key, value in required_vars.items() if not value]

        if missing_vars:
            raise ValueError(f"í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {missing_vars}")

        logging.info("âœ… ëª¨ë“  í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì™„ë£Œ")

    def _initialize_clients(self):
        """Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""

        try:
            # OpenAI í´ë¼ì´ì–¸íŠ¸
            self.openai_client = AzureOpenAI(
                azure_endpoint=self.azure_openai_endpoint,
                api_key=self.azure_openai_key,
                api_version="2024-02-01",
            )

            # AI Search í´ë¼ì´ì–¸íŠ¸
            self.search_client = SearchClient(
                endpoint=self.ai_search_endpoint,
                index_name=self.ai_search_index,
                credential=AzureKeyCredential(self.ai_search_admin_key),
            )

            # Table Storage í´ë¼ì´ì–¸íŠ¸
            self.table_service = TableServiceClient.from_connection_string(
                self.azure_storage_connection_string
            )

            logging.info("âœ… ëª¨ë“  Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logging.error(f"âŒ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    def _detect_embedding_deployment(self):
        """ì„ë² ë”© ë°°í¬ëª… ìë™ ê°ì§€"""

        # ì¼ë°˜ì ì¸ ì„ë² ë”© ë°°í¬ëª…ë“¤
        possible_names = [
            "text-embedding-3-small",
            "text-embedding-ada-002",
            "embedding-3-small",
            "embedding",
            self.azure_openai_deployment_emb,
        ]

        for deployment_name in possible_names:
            try:
                logging.info(f"ì„ë² ë”© ë°°í¬ëª… í…ŒìŠ¤íŠ¸: {deployment_name}")
                response = self.openai_client.embeddings.create(
                    model=deployment_name, input=["í…ŒìŠ¤íŠ¸"]
                )

                if response.data:
                    self.azure_openai_deployment_emb = deployment_name
                    logging.info(f"âœ… ì„ë² ë”© ë°°í¬ëª… í™•ì¸: {deployment_name}")
                    return

            except Exception as e:
                logging.warning(f"ë°°í¬ëª… '{deployment_name}' í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                continue

        # ëª¨ë“  ë°°í¬ëª… ì‹¤íŒ¨ì‹œ ì˜¤ë¥˜
        raise ValueError(
            "ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ë°°í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. AZURE_OPENAI_DEPLOYMENT_EMB í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        )

    def _pre_extract_deadlines(self, text: str, max_items: int = 5) -> List[str]:
        """
        ë³¸ë¬¸ì—ì„œ í•œêµ­ì–´ ê¸°í•œ í‘œí˜„ í›„ë³´ë¥¼ ë½‘ì•„ LLMì— íŒíŠ¸ë¡œ ì œê³µ.
        """
        patterns = [
            r"\(\s*\d{1,2}/\d{1,2}(?:\([^)]*\))?\s*ê¹Œì§€\s*\)",
            r"\d{1,2}/\d{1,2}(?:\([^)]*\))?\s*ê¹Œì§€",
            r"\d{4}-\d{1,2}-\d{1,2}(?:\s*\d{1,2}:\d{2})?\s*ê¹Œì§€",
            r"(?:ì´ë²ˆ\s*ì£¼|ê¸ˆì£¼)\s*(ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)ìš”ì¼?\s*ê¹Œì§€",
            r"(?:ê¸ˆì¼|ì˜¤ëŠ˜|ë‚´ì¼)\s*(?:ì˜¤ì „|ì˜¤í›„)?\s*\d{1,2}ì‹œ(?:\s*\d{1,2}ë¶„)?\s*ê¹Œì§€",
            r"(?:ì˜¤ì „|ì˜¤í›„)?\s*\d{1,2}ì‹œ(?:\s*\d{1,2}ë¶„)?\s*ê¹Œì§€",
            r"(?:ê¸ˆì¼|ì˜¤ëŠ˜|ë‚´ì¼)\s*ê¹Œì§€",
            # â–¼ 'ê¹Œì§€' ì—†ëŠ” í”í•œ í‘œí˜„
            r"ë§ˆê°[:\s]*\d{1,2}/\d{1,2}(?:\([^)]*\))?",
            r"\d{1,2}/\d{1,2}(?:\([^)]*\))?(?:\s*\d{1,2}:\d{2})?",
            r"\d{4}-\d{1,2}-\d{1,2}",
            r"\d+\s*ì¼\s*(?:í›„|ë’¤)",
            r"\b(?:EOD|EOW)\b",
            r"(ì—…ë¬´\s*(?:ì¢…ë£Œ|ì‹œê°„)\s*ì „)",
            r"\d{1,2}/\d{1,2}\s*~\s*\d{1,2}/\d{1,2}",
            r"\d{4}-\d{1,2}-\d{1,2}\s*~\s*\d{4}-\d{1,2}-\d{1,2}",
            # â–¼ ì£¼/ì›” ë‚´
            r"(ì´ë²ˆ\s*ì£¼\s*ë‚´|ì£¼ì¤‘|ì´ë²ˆ\s*ë‹¬\s*ë‚´|ì›”ë§\s*ê¹Œì§€|ë¶„ê¸°\s*ë§\s*ê¹Œì§€)",
        ]
        found = []
        for p in patterns:
            for m in re.finditer(p, text, flags=re.IGNORECASE):
                s = m.group(0).strip()
                if s not in found:
                    found.append(s)
                    if len(found) >= max_items:
                        return found
        return found

    def _collect_deadline_hints(self, email: Dict) -> List[str]:
        text_blob = f"{email.get('subject','')}\n\n{email.get('body','')}".strip()
        return self._pre_extract_deadlines(text_blob, max_items=10)

    def _find_context(self, text: str, snippet: str, width: int = 80) -> str:
        i = text.find(snippet)
        if i == -1:
            return ""
        start = max(0, i - width)
        end = min(len(text), i + len(snippet) + width)
        return text[start:end]

    def _is_due_for_user(self, text: str, cand: str, user_context: dict) -> bool:
        """
        'ë‚˜'ì—ê²Œ ìœ íš¨í•œ ë§ˆê°(due_raw)ì¸ì§€ íŒë³„.
        ê·œì¹™:
        1) (ê¸°ì¡´) ë‚´ ë©˜ì…˜ ~ ë‹¤ìŒ ë©˜ì…˜ ì‚¬ì´ êµ¬ê°„ì— candê°€ ìˆìœ¼ë©´ ë‚´ ê²ƒ.
        2) (ì‹ ê·œ) ì—¬ëŸ¬ ë©˜ì…˜ì´ í•œ ì¤„/ì§§ì€ ê°„ê²©(ê°™ì€ ë¬¸ì¥)ìœ¼ë¡œ ë¬¶ì¸ 'í´ëŸ¬ìŠ¤í„°' ì§í›„ì— candê°€ ë‚˜ì˜¤ë©´,
            ê·¸ í´ëŸ¬ìŠ¤í„°ì— ë‚´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë‚´ ê²ƒìœ¼ë¡œ ê°„ì£¼(ê³µë™ ì§€ì‹œ).
        3) (ë³´ê°•) cand ì§ì „ ìœˆë„ìš°ì—ì„œ ë§ˆì§€ë§‰ ë©˜ì…˜ì´ 'ë‚˜'ë¼ë©´ ë‚´ ê²ƒ.
        4) ë©˜ì…˜ì´ ì „í˜€ ì—†ìœ¼ë©´ ì´ì „ì˜ ì™„í™” ê·œì¹™ìœ¼ë¡œ íŒë‹¨.
        """
        name = (user_context.get("name") or "").strip()
        email = (user_context.get("email") or "").strip()
        team = (user_context.get("team") or "").strip()

        def _norm(s: str) -> str:
            return (s or "").replace(" ", "").lower()

        def _is_self_mention_text(mention_text: str) -> bool:
            # mention_text ì˜ˆ: "@ë°•ì§€í›ˆ(ë°±ì—”ë“œê°œë°œíŒ€)"
            base = mention_text.lstrip("@").split("(", 1)[0]
            packed = _norm(mention_text)
            return any(
                [
                    _norm(base) == _norm(name),
                    name and packed.startswith("@" + _norm(name)),
                    email and _norm(email) in packed,
                    team and _norm(team) in packed,
                ]
            )

        cand_idx = text.find(cand)
        if cand_idx == -1:
            return False

        # ëª¨ë“  ë©˜ì…˜ ìˆ˜ì§‘
        mention_re = r"@[A-Za-zê°€-í£0-9_.]+(?:\([^)]+\))?"
        mentions = list(re.finditer(mention_re, text))

        # ë©˜ì…˜ì´ ì—†ìœ¼ë©´: ì´ì „ ì™„í™” ê·œì¹™ ìœ ì§€
        if not mentions:
            ctx = self._find_context(text, cand, width=80)
            return any(
                [
                    name and (name in ctx),
                    email and (email in ctx),
                    team and (team in ctx),
                    re.search(
                        r"(ì•„ë˜\s*ì‘ì—…|ë‹¤ìŒ\s*ì‘ì—…).*(ê¹Œì§€|ë§ˆê°|ë¶€íƒ|ìš”ì²­|í™•ì¸)", ctx
                    ),
                ]
            )

        # 1) ê¸°ë³¸: ë‚´ ë©˜ì…˜ ~ ë‹¤ìŒ ë©˜ì…˜ ì‚¬ì´ êµ¬ê°„
        for i, m in enumerate(mentions):
            if _is_self_mention_text(m.group(0)):
                seg_start = m.end()
                seg_end = (
                    mentions[i + 1].start() if i + 1 < len(mentions) else len(text)
                )
                if seg_start <= cand_idx < seg_end:
                    return True

        # 2) ë©˜ì…˜ í´ëŸ¬ìŠ¤í„°(ê°™ì€ ë¬¸ì¥/ì§§ì€ ê°„ê²©) ì§í›„ cand â†’ í´ëŸ¬ìŠ¤í„°ì— ë‚´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True
        #    - ê°™ì€ ì¤„(ê°œí–‰ ì—†ìŒ) & ê°„ê²© â‰¤ 80ìë©´ ë™ì¼ í´ëŸ¬ìŠ¤í„°ë¡œ ê°„ì£¼
        CLUSTER_GAP = 80

        # cand ë°”ë¡œ ì•ì˜ ë§ˆì§€ë§‰ ë©˜ì…˜ ì¸ë±ìŠ¤
        last_before_idx = -1
        for i, m in enumerate(mentions):
            if m.start() < cand_idx:
                last_before_idx = i
            else:
                break

        if last_before_idx >= 0:
            # ë’¤ë¡œ ëª¨ìœ¼ë©° ê°™ì€ ì¤„ & ì§§ì€ ê°„ê²©ì¸ ë©˜ì…˜ë“¤ì„ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ê¸°
            cluster = [mentions[last_before_idx]]
            j = last_before_idx - 1
            while j >= 0:
                prev = mentions[j]
                gap_text = text[prev.end() : cluster[0].start()]
                if ("\n" not in gap_text) and (len(gap_text) <= CLUSTER_GAP):
                    cluster.insert(0, prev)
                    j -= 1
                else:
                    break

            # í´ëŸ¬ìŠ¤í„° ë ~ cand ì‚¬ì´ì— ë‹¤ë¥¸ ë©˜ì…˜ì´ ë¼ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
            cluster_end = cluster[-1].end()
            has_mention_between = any(
                m.start() >= cluster_end and m.start() < cand_idx for m in mentions
            )
            if not has_mention_between:
                # í´ëŸ¬ìŠ¤í„° ë‚´ì— ë‚´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê³µë™ ì§€ì‹œë¡œ ê°„ì£¼
                if any(_is_self_mention_text(m.group(0)) for m in cluster):
                    return True

        # 3) cand ì§ì „ ìœˆë„ìš°(200ì ë˜ëŠ” 2ì¤„)ì—ì„œ "ë§ˆì§€ë§‰ ë©˜ì…˜ == ë‚´ ë©˜ì…˜"ì´ë©´ True
        window_start = max(0, cand_idx - 200)
        ctx = text[window_start:cand_idx]
        # ë§ˆì§€ë§‰ ë©˜ì…˜ ì°¾ê¸°
        last_any = None
        for m in re.finditer(mention_re, ctx):
            last_any = m
        if last_any:
            if _is_self_mention_text(last_any.group(0)):
                # ë™ì¼ ì¤„ ë‚´ ì§€ì‹œ í‘œí˜„ì´ ë¶™ì–´ ìˆìœ¼ë©´ ë”ìš± ê°•í•˜ê²Œ ì°¸ìœ¼ë¡œ ë³¸ë‹¤
                tail = ctx[last_any.end() :]
                if ("\n" not in tail) or re.search(
                    r"(ê¹Œì§€|ë§ˆê°|ë¶€íƒ|ìš”ì²­|í™•ì¸|ì™„ë£Œ)", tail
                ):
                    return True

        # ìµœì¢… ì‹¤íŒ¨ â†’ ë‚´ due ì•„ë‹˜
        return False

    def _sanitize_document_key(self, key: str) -> str:
        """Azure Search ë¬¸ì„œ í‚¤ ì •ì œ"""

        # Azure Search í‚¤ ê·œì¹™: ë¬¸ì, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´(_), ëŒ€ì‹œ(-), ë“±í˜¸(=)ë§Œ í—ˆìš©
        # íŠ¹ìˆ˜ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        sanitized = re.sub(r"[^a-zA-Z0-9_\-=]", "_", key)

        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ ì •ë¦¬
        sanitized = re.sub(r"_+", "_", sanitized)

        # ì‹œì‘/ë ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        sanitized = sanitized.strip("_")

        # ê¸¸ì´ ì œí•œ (Azure Search í‚¤ ìµœëŒ€ ê¸¸ì´: 1024ì)
        if len(sanitized) > 1000:
            # í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥
            hash_suffix = hashlib.md5(key.encode()).hexdigest()[:8]
            sanitized = sanitized[:992] + "_" + hash_suffix

        return sanitized

    def load_email_data(self, file_path: str) -> List[Dict]:
        """ì´ë©”ì¼ JSON íŒŒì¼ ë¡œë“œ"""

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            emails = data.get("values", [])
            logging.info(f"ğŸ“§ {len(emails)}ê°œ ì´ë©”ì¼ ë¡œë“œ ì™„ë£Œ: {file_path}")
            return emails

        except Exception as e:
            logging.error(f"âŒ ì´ë©”ì¼ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def _html_to_text(self, html_str: str) -> str:
        if not html_str:
            return ""
        # ì•„ì£¼ ê°€ë²¼ìš´ ë³€í™˜(BeautifulSoup ì—†ì´)
        text = re.sub(
            r"(?is)<(script|style).*?>.*?</\1>", " ", html_str
        )  # ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ ì œê±°
        text = re.sub(r"(?is)<br\s*/?>", "\n", text)
        text = re.sub(r"(?is)</p>", "\n", text)
        text = re.sub(r"(?is)</li>", "\n- ", text)
        text = re.sub(r"(?is)<[^>]+>", " ", text)  # íƒœê·¸ ì œê±°
        text = html.unescape(text)
        text = re.sub(r"[ \t\u00A0]+", " ", text)
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()

    def preprocess_email(self, email_data: Dict) -> Dict:
        """ì´ë©”ì¼ ë°ì´í„° ì „ì²˜ë¦¬ (ì•ˆì „í•œ ì²˜ë¦¬)"""

        # ì•ˆì „í•œ í•„ë“œ ì¶”ì¶œ
        def safe_get(key: str, default: str = "") -> str:
            value = email_data.get(key)
            return str(value) if value is not None else default

        def safe_get_list(key: str, default_list: List = None) -> List:
            value = email_data.get(key)
            if isinstance(value, list):
                return value
            return default_list or []

        def _html_to_text(self, html_str: str) -> str:
            if not html_str:
                return ""
            import html as _html

            text = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", html_str)
            text = re.sub(r"(?is)<br\s*/?>", "\n", text)
            text = re.sub(r"(?is)</p>", "\n", text)
            text = re.sub(r"(?is)</li>", "\n- ", text)
            text = re.sub(r"(?is)<[^>]+>", " ", text)
            text = _html.unescape(text)
            text = re.sub(r"[ \t\u00A0]+", " ", text)
            text = re.sub(r"\n{3,}", "\n\n", text)
            return text.strip()

        # ê¸°ë³¸ ì •ì œ
        body = safe_get("email_body")
        html_body = safe_get("html_body")

        # âœ… í•­ìƒ ë³‘í•© (ì¤‘ë³µ ì¤„ ì œê±°)
        if html_body:
            html_text = self._html_to_text(html_body)
            if html_text:
                merged = (body + "\n\n" + html_text).strip() if body else html_text
                # ì¤‘ë³µ ë¼ì¸ ê°„ë‹¨ ì œê±°
                lines = []
                seen = set()
                for ln in merged.splitlines():
                    key = ln.strip()
                    if key and key not in seen:
                        lines.append(ln)
                        seen.add(key)
                body = "\n".join(lines)

        # ì„œëª…/ê´‘ê³  ë¸”ë¡ ì œê±° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        signature_patterns = [r"\n\n--\n.*", r"\n\n.*ë“œë¦¼$", r"\n\n.*ê°ì‚¬í•©ë‹ˆë‹¤\..*"]

        for pattern in signature_patterns:
            body = re.sub(pattern, "", body, flags=re.DOTALL | re.MULTILINE)

        # ì•ˆì „í•œ ë°°ì—´ ì²˜ë¦¬
        to_names = safe_get_list("to_names")
        to_addresses = safe_get_list("to_addresses")
        cc_names = safe_get_list("cc_names")
        cc_addresses = safe_get_list("cc_addresses")

        # ê¸¸ì´ ë§ì¶”ê¸° (namesì™€ addresses ë°°ì—´ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        max_to_len = max(len(to_names), len(to_addresses))
        max_cc_len = max(len(cc_names), len(cc_addresses))

        # to ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        to_list = []
        for i in range(max_to_len):
            name = to_names[i] if i < len(to_names) else ""
            email = to_addresses[i] if i < len(to_addresses) else ""
            if name or email:  # ì ì–´ë„ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
                to_list.append({"name": name, "email": email})

        # cc ë¦¬ìŠ¤íŠ¸ ì •ê·œí™”
        cc_list = []
        for i in range(max_cc_len):
            name = cc_names[i] if i < len(cc_names) else ""
            email = cc_addresses[i] if i < len(cc_addresses) else ""
            if name or email:  # ì ì–´ë„ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•¨
                cc_list.append({"name": name, "email": email})

        # threads ë°ì´í„° ì•ˆì „í•œ ì²˜ë¦¬
        threads = email_data.get("threads", {})
        keywords = []
        if isinstance(threads, dict):
            keywords = threads.get("keywords", [])
            if not isinstance(keywords, list):
                keywords = []

        # í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        standardized = {
            "recordId": safe_get("recordId"),
            "emailId": safe_get("email_id"),
            "subject": safe_get("subject"),
            "from": {"name": safe_get("from_name"), "email": safe_get("from_address")},
            "to": to_list,
            "cc": cc_list,
            "receivedAt": safe_get("date"),
            "body": body.strip(),
            "html_body": html_body,
            "conversationId": safe_get("thread_id"),
            "priority_hint": safe_get("priority"),
            "keywords": keywords,
        }

        return standardized

    def analyze_with_policy_engine(self, email_data: Dict, user_context: Dict) -> Dict:
        """ì •ì±… ì—”ì§„ ë¶„ì„ (ì•ˆì „í•œ ì²˜ë¦¬)"""

        # ì•ˆì „í•œ í•„ë“œ ì¶”ì¶œ
        from_email = email_data.get("from_address", "")
        to_emails = email_data.get("to_addresses", [])
        cc_emails = email_data.get("cc_addresses", [])
        body = email_data.get("email_body", "")

        # None ì²´í¬ ë° ê¸°ë³¸ê°’ ì„¤ì •
        if not isinstance(to_emails, list):
            to_emails = []
        if not isinstance(cc_emails, list):
            cc_emails = []
        if not isinstance(body, str):
            body = ""
        if not isinstance(from_email, str):
            from_email = ""

        user_name = user_context.get("name", "")
        user_email = user_context.get("email", "")
        user_team = user_context.get("team", "")

        # ë©˜ì…˜ ì¶”ì¶œ (ë¹ˆ ë¬¸ìì—´ ì²´í¬)
        mentions = []
        if body:
            try:
                mentions = re.findall(r"@(\S+(?:\([^)]+\))?)", body)
                mentions = [f"@{mention}" for mention in mentions]
            except Exception as e:
                logging.warning(f"ë©˜ì…˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                mentions = []

        # ìš”ì²­ í‚¤ì›Œë“œ ê°ì§€
        request_keywords = [
            "ë¶€íƒ",
            "ìš”ì²­",
            "í™•ì¸",
            "ê²€í† ",
            "ìŠ¹ì¸",
            "íšŒì‹ ",
            "ì¦‰ì‹œ",
            "ê¸´ê¸‰",
            "ë§ˆê°",
            "ì™„ë£Œ",
            "í•´ì£¼ì„¸ìš”",
            "ë°”ëë‹ˆë‹¤",
            "ì²˜ë¦¬",
            "ëŒ€ì‘",
            "ë¶„ì„",
            "ì ê²€",
            "ì‹¤í–‰",
        ]
        request_detected = False
        if body:
            try:
                request_detected = any(keyword in body for keyword in request_keywords)
            except Exception as e:
                logging.warning(f"ìš”ì²­ í‚¤ì›Œë“œ ê°ì§€ ì‹¤íŒ¨: {e}")
                request_detected = False

        # ê¸°ë³¸ íŒì • ìš”ì†Œ (ì•ˆì „í•œ ë¹„êµ)
        self_sent = bool(from_email and user_email and from_email == user_email)
        to_contains_self = bool(user_email and user_email in to_emails)
        cc_contains_self = bool(user_email and user_email in cc_emails)

        # ì •ì±… ê²°ì •
        policy_decision = "none"

        try:
            if to_contains_self and request_detected:
                # A: Toì— í¬í•¨ + ìš”ì²­ í‘œí˜„
                user_mentions = [
                    mention
                    for mention in mentions
                    if user_name and f"@{user_name}" in mention
                ]
                if (
                    not any(
                        mention for mention in mentions if mention != f"@{user_name}"
                    )
                    or user_mentions
                ):
                    policy_decision = "A"
            elif cc_contains_self and not to_contains_self:
                # B: CCì—ë§Œ í¬í•¨
                if any(
                    user_name and f"@{user_name}" in mention for mention in mentions
                ):
                    policy_decision = "A"  # ëª…ì‹œì  ì§€ëª©ì´ë©´ ì•¡ì…˜
                else:
                    policy_decision = "B"  # ë¹„ì•¡ì…˜
            elif self_sent and request_detected:
                # C: ë³¸ì¸ì´ ë³´ë‚¸ ìš”ì²­
                policy_decision = "C"
            elif (
                to_contains_self
                and user_team
                and user_team in body
                and request_detected
            ):
                # D: íŒ€ ë‹¨ìœ„ ìš”ì²­
                policy_decision = "D"
        except Exception as e:
            logging.warning(f"ì •ì±… ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}")
            policy_decision = "none"

        return {
            "policy_decision": policy_decision,
            "self_sent": self_sent,
            "to_contains_self": to_contains_self,
            "cc_contains_self": cc_contains_self,
            "mentions": mentions,
            "request_detected": request_detected,
        }

    def extract_actions_with_llm(
        self, email_data: Dict, policy_signals: Dict, user_context: Dict
    ) -> Dict:
        """LLMì„ ì‚¬ìš©í•œ ì•¡ì…˜ ì¶”ì¶œ"""

        schema_block = """
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ, í•œ ì¤„ì˜ ìœ íš¨í•œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {
        "is_action": true,
        "policy_decision": "A|B|C|D|none",
        "action": {
            "type": "DO|FOLLOW_UP|NONE",
            "title": "ì•¡ì…˜ ì œëª©(20ì ì´ë‚´)",
            "assignee_candidates": ["ì´ë¦„ <ì´ë©”ì¼>", "íŒ€ëª…"],
            "due_raw": "ì›ë³¸ ê¸°í•œ í‘œí˜„(íŒíŠ¸ì—ì„œ ê³ ë¥´ê±°ë‚˜ ë³¸ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë°œì·Œ; ì—†ìœ¼ë©´ null)",
            "priority": "High|Medium|Low",
            "tags": ["íƒœê·¸1", "íƒœê·¸2"],
            "rationale": "íŒë‹¨ ê·¼ê±°(1~2ë¬¸ì¥)"
        }
        }
        ì£¼ì˜:
        - ë¬´ì¡°ê±´ JSONë§Œ ì¶œë ¥(ì½”ë“œë¸”ë¡, ì„¤ëª… ê¸ˆì§€)
        - ê°’ì´ ì—†ìœ¼ë©´ null ë¡œ ì±„ì›Œë¼
        - due_raw ëŠ” ë°˜ë“œì‹œ 'ì›ë¬¸ í‘œí˜„'ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬
        """.strip()

        system_prompt = f"""
            ë‹¹ì‹ ì€ ì´ë©”ì¼ì—ì„œ ì•¡ì…˜ ì•„ì´í…œì„ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

            ë‹¤ìŒ ì •ì±… ê·œì¹™ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”:
            - A: Toì— í¬í•¨ + ìš”ì²­ í‘œí˜„ â†’ DO ì•¡ì…˜
            - B: CCì—ë§Œ í¬í•¨ + ëª…ì‹œì  ì§€ëª© ì—†ìŒ â†’ ë¹„ì•¡ì…˜
            - C: ë³¸ì¸ì´ ë³´ë‚¸ ìš”ì²­ â†’ FOLLOW_UP ì•¡ì…˜
            - D: íŒ€ ë‹¨ìœ„ ìš”ì²­ + Toì— í¬í•¨ â†’ DO ì•¡ì…˜

            ì‚¬ìš©ì ì •ë³´:
            - ì´ë¦„: {user_context['name']}
            - ì´ë©”ì¼: {user_context['email']}
            - íŒ€: {user_context['team']}

            JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
            """.strip()

        deadline_hints = self._collect_deadline_hints(email_data)

        user_prompt = f"""
            ì´ë©”ì¼ ë¶„ì„:

            ì œëª©: {email_data['subject']}
            ë°œì‹ ì: {email_data['from']['name']} <{email_data['from']['email']}>
            ìˆ˜ì‹ ì: {', '.join([f"{p['name']} <{p['email']}>" for p in email_data['to']])}
            ì°¸ì¡°: {', '.join([f"{p['name']} <{p['email']}>" for p in email_data['cc']])}
            ë‚ ì§œ: {email_data['receivedAt']}

            ë³¸ë¬¸:
            {email_data['body'][:3000]}

            [ê¸°í•œ í›„ë³´ íŒíŠ¸] ë³¸ë¬¸ì—ì„œ ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¯¸ë¦¬ íƒì§€ëœ í‘œí˜„ë“¤:
            {deadline_hints}

            ì •ì±… ì‹ í˜¸:
            - ì •ì±… ê²°ì •: {policy_signals['policy_decision']}
            - ë³¸ì¸ ë°œì†¡: {policy_signals['self_sent']}
            - Toì— ë³¸ì¸ í¬í•¨: {policy_signals['to_contains_self']}
            - ë©˜ì…˜: {policy_signals['mentions']}
            - ìš”ì²­ ê°ì§€: {policy_signals['request_detected']}
            - titleì€ 12~20ì í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ, ë‚˜ì—ê²Œ í• ë‹¹ëœ í•µì‹¬ ì‘ì—…ì„ ë™ì‚¬+ëª…ì‚¬ë¡œ ìš”ì•½(ì˜ˆ: "API ì„œë²„ ë¡œê·¸ ë¶„ì„").
            - [ê¸°í•œ í›„ë³´ íŒíŠ¸]ê°€ ë¹„ì–´ ìˆì–´ë„, ë³¸ë¬¸/ì œëª©ì—ì„œ ì§ì ‘ ë‚ ì§œÂ·ìš”ì¼Â·ì‹œê°„Â·ë²”ìœ„ë¥¼ ì°¾ì•„ due_rawì— 'ì›ë¬¸ ê·¸ëŒ€ë¡œ' ë³µì‚¬í•´ë¼. ì •ë§ ì›ë¬¸ì— ì•„ë¬´ í‘œí˜„ë„ ì—†ì„ ë•Œë§Œ nullì„ ì‚¬ìš©í•œë‹¤.

            {schema_block}
            """.strip()

        try:
            logging.info("=== ğŸ“¤ LLM ìš”ì²­ (system) ===\n%s", system_prompt)
            logging.info("=== ğŸ“¤ LLM ìš”ì²­ (user) ===\n%s", user_prompt)

            response = self.openai_client.chat.completions.create(
                model=self.azure_openai_deployment_chat,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.3,
                max_tokens=800,
            )

            response_text = response.choices[0].message.content.strip()
            logging.info("=== ğŸ“¥ LLM ì›ë³¸ ì‘ë‹µ ===\n%s", response_text)

            result = json.loads(response_text)

            text_blob = f"{email_data.get('subject','')}\n\n{email_data.get('body','')}"
            action = result.get("action") or {}
            due_raw = (action.get("due_raw") or "").strip()

            # 'ë‚˜'ì—ê²Œ í• ë‹¹ëœ ê¸°í•œì¸ì§€ í™•ì¸
            if (
                due_raw
                and not result.get("type") == "FOLLOW_UP"
                and not self._is_due_for_user(text_blob, due_raw, user_context)
            ):
                logging.info("ğŸš« íƒ€ì¸ ì§€ì‹œ ë§¥ë½ìœ¼ë¡œ due_raw ë¬´íš¨í™”: %s", due_raw)
                action["due_raw"] = None
                result["action"] = action

            logging.info("âœ… LLM ì•¡ì…˜ ì¶”ì¶œ ì™„ë£Œ: %s", result.get("is_action", False))
            return result

        except json.JSONDecodeError as e:
            logging.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨: %s", e)
            logging.error(
                "LLM ì›ë³¸ ì‘ë‹µ:\n%s",
                response_text if "response_text" in locals() else "N/A",
            )
            return {"is_action": False, "policy_decision": "none", "action": None}
        except Exception as e:
            logging.exception("âŒ LLM ì¶”ì¶œ ì‹¤íŒ¨")
            return {"is_action": False, "policy_decision": "none", "action": None}

    def normalize_action(self, raw_action: Dict, email_data: Dict) -> Optional[Dict]:
        """ì•¡ì…˜ ë°ì´í„° ì •ê·œí™” (ê·œì¹™â†’LLM ë³´ì •ìœ¼ë¡œ due í•´ì„, KST/UTC ë™ì‹œ ì œê³µ)"""

        if not raw_action.get("is_action") or not raw_action.get("action"):
            return None

        action = raw_action["action"]
        due_raw = (action.get("due_raw") or "").strip()

        # ë‹´ë‹¹ì ê²°ì •
        assignee = "ë¯¸ì§€ì •"
        for cand in action.get("assignee_candidates") or []:
            if "@" in cand:
                assignee = cand
                break

        # ê¸°ë³¸ ì‹ ë¢°ë„
        confidence = self.default_confidence

        # 0) LLM ë‹¨ê³„ì—ì„œ ì´ë¯¸ ë„£ì–´ë‘” í•´ì„ê°’ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        due_iso = action.get("due_resolved_iso")
        due_kst_str = action.get("due_resolved_kst")

        # 1) ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜(+LLM ë³´ì • fallback)ìœ¼ë¡œ í•´ì„
        if not due_iso and due_raw:
            rkst, risco = self._resolve_relative_deadline(
                due_raw, email_data.get("receivedAt")
            )
            if risco:
                due_iso = risco
                due_kst_str = rkst
                action["due_resolved_iso"] = risco
                action["due_resolved_kst"] = rkst

        # 2) ì—¬ì „íˆ ì—†ìœ¼ë©´(ì•„ì£¼ ì˜ˆì™¸) ê¸°ì¡´ íŒŒì‹± ë°±ì—…
        if not due_iso and due_raw:
            try:
                kst = ZoneInfo("Asia/Seoul")
                now_kst = datetime.now(kst)
                hour = 18
                minute = 0
                t = re.search(r"(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})ì‹œ(?:\s*(\d{1,2})ë¶„)?", due_raw)
                if t:
                    ampm, hh, mm = t.groups()
                    hour = int(hh)
                    minute = int(mm) if mm else 0
                    if ampm == "ì˜¤í›„" and hour < 12:
                        hour += 12
                    if ampm == "ì˜¤ì „" and hour == 12:
                        hour = 0

                target_date = None
                if re.search(r"(ê¸ˆì¼|ì˜¤ëŠ˜)", due_raw):
                    target_date = now_kst.date()
                elif "ë‚´ì¼" in due_raw or "ëª…ì¼" in due_raw:
                    target_date = (now_kst + timedelta(days=1)).date()
                elif re.search(
                    r"(?:ì´ë²ˆ\s*ì£¼|ê¸ˆì£¼)\s*(ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)ìš”ì¼?\s*ê¹Œì§€", due_raw
                ):
                    wd_map = {
                        "ì›”": 0,
                        "í™”": 1,
                        "ìˆ˜": 2,
                        "ëª©": 3,
                        "ê¸ˆ": 4,
                        "í† ": 5,
                        "ì¼": 6,
                    }
                    wd = re.search(
                        r"(?:ì´ë²ˆ\s*ì£¼|ê¸ˆì£¼)\s*(ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)", due_raw
                    ).group(1)
                    delta = (wd_map[wd] - now_kst.weekday()) % 7
                    target_date = (now_kst + timedelta(days=delta)).date()
                elif re.search(r"\d{4}-\d{1,2}-\d{1,2}", due_raw):
                    y, m, d = map(
                        int, re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", due_raw).groups()
                    )
                    target_date = datetime(y, m, d, tzinfo=kst).date()
                elif re.search(r"\b\d{1,2}/\d{1,2}\b", due_raw):
                    m, d = map(
                        int, re.search(r"\b(\d{1,2})/(\d{1,2})\b", due_raw).groups()
                    )
                    y = now_kst.year if m >= now_kst.month else now_kst.year + 1
                    target_date = datetime(y, m, d, tzinfo=kst).date()
                elif re.search(r"\d+\s*ì¼\s*(?:í›„|ë’¤)", due_raw):
                    days = int(re.search(r"(\d+)\s*ì¼\s*(?:í›„|ë’¤)", due_raw).group(1))
                    target_date = (now_kst + timedelta(days=days)).date()

                if not target_date:
                    try:
                        parsed = parser.parse(due_raw, fuzzy=True)
                        parsed = (
                            parsed.astimezone(kst)
                            if parsed.tzinfo
                            else parsed.replace(tzinfo=kst)
                        )
                        target_date = parsed.date()
                        hour = parsed.hour or hour
                        minute = parsed.minute or minute
                    except Exception:
                        pass

                if target_date:
                    due_kst = datetime.combine(
                        target_date, dt_time(hour, minute, tzinfo=kst)
                    )
                    due_iso = due_kst.astimezone(timezone.utc).isoformat()
                    due_kst_str = due_kst.strftime("%Y-%m-%d %H:%M KST")
                    action.setdefault("due_resolved_kst", due_kst_str)
                    action.setdefault("due_resolved_iso", due_iso)
            except Exception as e:
                logging.error(f"ë‚ ì§œ/ì‹œê°„ ì •ê·œí™” ì˜¤ë¥˜: {e}, due_raw: {due_raw}")
                due_iso = None

        # ì‹ ë¢°ë„ ë³´ì •
        if action.get("type") == "DO" and due_iso and "@" in assignee:
            confidence = min(confidence + 0.2, 1.0)
        elif action.get("type") == "FOLLOW_UP" and due_iso:
            confidence = min(confidence + 0.15, 1.0)

        # ë…¸íŠ¸
        note_parts = []
        if due_raw:
            note_parts.append(f"ì›ë³¸ ê¸°í•œ: {due_raw}")
        if due_kst_str:
            note_parts.append(f"í•´ì„(KST): {due_kst_str}")

        return {
            "title": action.get("title", ""),
            "assignee": assignee,
            "due": due_iso,
            "priority": action.get("priority", "Medium"),
            "tags": action.get("tags", []),
            "type": action.get("type", "DO"),
            "confidence": confidence,
            "notes": " | ".join(note_parts) if note_parts else "",
        }

    def create_text_chunks(
        self, text: str, chunk_size: int = 900, overlap: int = 150
    ) -> List[str]:
        """í…ìŠ¤íŠ¸ ì²­í‚¹"""

        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            if end < len(text):
                last_period = text.rfind(".", start, end)
                last_newline = text.rfind("\n", start, end)

                boundary = max(last_period, last_newline)
                if boundary > start + chunk_size // 2:
                    end = boundary + 1

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = end - overlap

        return chunks

    def _llm_resolve_deadline(
        self, due_raw: str, received_at_iso: Optional[str]
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        ê·œì¹™ íŒŒì‹±ì´ ì•ˆ ë  ë•Œ LLMë¡œ ìƒëŒ€í‘œí˜„ì„ ì ˆëŒ€ì‹œê°„ìœ¼ë¡œ ë³´ì •.
        ë°˜í™˜: (resolved_kst_str "YYYY-MM-DD HH:MM KST", resolved_utc_iso) ë˜ëŠ” (None, None)
        """
        try:
            kst = ZoneInfo("Asia/Seoul")
            now_kst = None
            if received_at_iso:
                tmp = parser.parse(received_at_iso)
                now_kst = tmp.astimezone(kst) if tmp.tzinfo else tmp.replace(tzinfo=kst)
            if not now_kst:
                now_kst = datetime.now(kst)

            system_prompt = (
                "ë„ˆëŠ” í•œêµ­ì–´ ê¸°í•œ í‘œí˜„ì„ KST ê¸°ì¤€ì˜ ëª…í™•í•œ ë‚ ì§œ/ì‹œê°„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„ìš°ë¯¸ì•¼.\n"
                '- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í•œ ì¤„: {"kst":"YYYY-MM-DD HH:MM","iso":"YYYY-MM-DDTHH:MM:SSZ"}\n'
                "- ì‹œê°„ì´ ì—†ìœ¼ë©´ 18:00ìœ¼ë¡œ ê°€ì •.\n"
                "- 'ê¸ˆì¼/ì˜¤ëŠ˜'=ìˆ˜ì‹ ì¼, 'ëª…ì¼/ë‚´ì¼'=+1, 'ëª¨ë ˆ'=+2.\n"
                "- 'ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼'=ìˆ˜ì‹ ì¼ì´ ì†í•œ ì£¼ì˜ ê¸ˆìš”ì¼.\n"
                "- 'ë‹¤ìŒ ì£¼/ì°¨ì£¼ í™”ìš”ì¼'=ë‹¤ìŒ ì£¼ì˜ í™”ìš”ì¼.\n"
                "- ë¶ˆê°€ëŠ¥í•˜ë©´ ë‘ ê°’ ëª¨ë‘ null."
            )
            user_prompt = (
                f"ì›ë¬¸: {due_raw}\n"
                f"ìˆ˜ì‹ ì‹œê°(KST): {now_kst.strftime('%Y-%m-%d %H:%M:%S')}\n"
                "í•œ ì¤„ JSONìœ¼ë¡œë§Œ ë‹µí•´."
            )

            resp = self.openai_client.chat.completions.create(
                model=self.azure_openai_deployment_chat,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.0,
                max_tokens=120,
            )
            raw = (resp.choices[0].message.content or "").strip()
            data = json.loads(raw)

            kst_str = data.get("kst")
            iso = data.get("iso")
            if (
                kst_str
                and re.match(r"^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}$", kst_str)
                and iso
                and iso.endswith("Z")
            ):
                return f"{kst_str} KST", iso
        except Exception as e:
            logging.info(f"LLM ê¸°í•œ ë³´ì • ì‹¤íŒ¨: {e}")
        return None, None

    def _resolve_relative_deadline(
        self, due_raw: str, received_at_iso: Optional[str]
    ) -> Tuple[Optional[str], Optional[str]]:
        """
        ìƒëŒ€/ëª¨í˜¸ í‘œí˜„(due_raw)ì„ KST/UTCë¡œ í•´ì„.
        ìš°ì„  ê·œì¹™ìœ¼ë¡œ ì‹œë„, ì‹¤íŒ¨ ì‹œ _llm_resolve_deadline()ìœ¼ë¡œ ë³´ì •.
        ë°˜í™˜: (resolved_kst_str 'YYYY-MM-DD HH:MM KST', resolved_utc_iso)
        """
        if not due_raw:
            return None, None

        kst = ZoneInfo("Asia/Seoul")
        # ê¸°ì¤€ì‹œê°: ìˆ˜ì‹ ì‹œê°ì´ ìˆìœ¼ë©´ ê·¸ê²ƒ, ì—†ìœ¼ë©´ now
        try:
            if received_at_iso:
                base = parser.parse(received_at_iso)
                now_kst = (
                    base.astimezone(kst) if base.tzinfo else base.replace(tzinfo=kst)
                )
            else:
                now_kst = datetime.now(kst)
        except Exception:
            now_kst = datetime.now(kst)

        text = due_raw.strip()

        # ê¸°ë³¸ ì‹œê°„(ë¯¸ì§€ì • ì‹œ 18:00)
        hour = 18
        minute = 0

        # ì˜¤ì „/ì˜¤í›„ ì‹œ:ë¶„
        t = re.search(r"(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2})ì‹œ(?:\s*(\d{1,2})ë¶„)?", text)
        if t:
            ampm, hh, mm = t.groups()
            hour = int(hh)
            minute = int(mm) if mm else 0
            if ampm == "ì˜¤í›„" and hour < 12:
                hour += 12
            if ampm == "ì˜¤ì „" and hour == 12:
                hour = 0

        wd_map = {"ì›”": 0, "í™”": 1, "ìˆ˜": 2, "ëª©": 3, "ê¸ˆ": 4, "í† ": 5, "ì¼": 6}
        target_date = None

        # ì˜¤ëŠ˜/ê¸ˆì¼/ëª…ì¼/ë‚´ì¼/ëª¨ë ˆ
        if re.search(r"(ê¸ˆì¼|ì˜¤ëŠ˜)", text):
            target_date = now_kst.date()
        elif re.search(r"(ëª…ì¼|ë‚´ì¼)", text):
            target_date = (now_kst + timedelta(days=1)).date()
        elif "ëª¨ë ˆ" in text:
            target_date = (now_kst + timedelta(days=2)).date()

        # ì´ë²ˆ ì£¼ ìš”ì¼ê¹Œì§€
        if not target_date:
            m = re.search(
                r"(?:ì´ë²ˆ\s*ì£¼|ê¸ˆì£¼)\s*(ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)ìš”ì¼?\s*ê¹Œì§€?", text
            )
            if m:
                wd = m.group(1)
                delta = (wd_map[wd] - now_kst.weekday()) % 7
                target_date = (now_kst + timedelta(days=delta)).date()

        # ë‹¤ìŒ ì£¼/ì°¨ì£¼ ìš”ì¼ê¹Œì§€
        if not target_date:
            m = re.search(
                r"(?:ë‹¤ìŒ\s*ì£¼|ì°¨ì£¼)\s*(ì›”|í™”|ìˆ˜|ëª©|ê¸ˆ|í† |ì¼)ìš”ì¼?\s*ê¹Œì§€?", text
            )
            if m:
                wd = m.group(1)
                # ë‹¤ìŒ ì£¼ ì›”ìš”ì¼
                delta_to_monday = (0 - now_kst.weekday()) % 7
                next_monday = (
                    now_kst + timedelta(days=delta_to_monday)
                ).date() + timedelta(days=7)
                target_date = next_monday + timedelta(days=wd_map[wd])

        # EOD/EOW
        if not target_date:
            if re.search(r"\bEOD\b", text, re.IGNORECASE):
                target_date = now_kst.date()
                hour, minute = 18, 0
            elif re.search(r"\bEOW\b", text, re.IGNORECASE):
                delta = (4 - now_kst.weekday()) % 7  # ê¸ˆìš”ì¼
                target_date = (now_kst + timedelta(days=delta)).date()
                hour, minute = 18, 0

        # YYYY-MM-DD
        if not target_date:
            m = re.search(r"(\d{4})-(\d{1,2})-(\d{1,2})", text)
            if m:
                y, mo, d = map(int, m.groups())
                target_date = datetime(y, mo, d, tzinfo=kst).date()

        # MM/DD
        if not target_date:
            m = re.search(r"\b(\d{1,2})/(\d{1,2})\b", text)
            if m:
                mo, d = map(int, m.groups())
                y = now_kst.year if mo >= now_kst.month else now_kst.year + 1
                target_date = datetime(y, mo, d, tzinfo=kst).date()

        # Nì¼ í›„/ë’¤
        if not target_date:
            m = re.search(r"(\d+)\s*ì¼\s*(?:í›„|ë’¤)", text)
            if m:
                days = int(m.group(1))
                target_date = (now_kst + timedelta(days=days)).date()

        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: dateutil
        if not target_date:
            try:
                parsed = parser.parse(text, fuzzy=True)
                parsed = (
                    parsed.astimezone(kst)
                    if parsed.tzinfo
                    else parsed.replace(tzinfo=kst)
                )
                target_date = parsed.date()
                hour = parsed.hour or hour
                minute = parsed.minute or minute
            except Exception:
                pass

        # ê·œì¹™ìœ¼ë¡œë„ ëª» êµ¬í•˜ë©´ LLM ë³´ì •
        if not target_date:
            return self._llm_resolve_deadline(
                due_raw=text, received_at_iso=received_at_iso
            )

        due_kst = datetime.combine(target_date, dt_time(hour, minute, tzinfo=kst))
        due_utc_iso = due_kst.astimezone(timezone.utc).isoformat()
        resolved_kst_str = due_kst.strftime("%Y-%m-%d %H:%M KST")
        return resolved_kst_str, due_utc_iso

    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""

        try:
            response = self.openai_client.embeddings.create(
                model=self.azure_openai_deployment_emb, input=texts
            )

            embeddings = [data.embedding for data in response.data]
            logging.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
            return embeddings

        except Exception as e:
            logging.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì„ë² ë”© ì‹¤íŒ¨ì‹œ 0ìœ¼ë¡œ ì±„ìš´ ë”ë¯¸ ë²¡í„° ë°˜í™˜
            return [[0.0] * 1536] * len(texts)

    def upload_to_search(self, email_data: Dict, action_data: Optional[Dict]) -> None:
        """Azure AI Searchì— ë¬¸ì„œ ì—…ë¡œë“œ"""

        # í…ìŠ¤íŠ¸ ì²­í‚¹
        full_text = f"{email_data['subject']}\n\n{email_data['body']}"
        chunks = self.create_text_chunks(full_text)

        # ì„ë² ë”© ìƒì„±
        embeddings = self.get_embeddings(chunks)

        # ê²€ìƒ‰ ë¬¸ì„œ ìƒì„±
        documents = []

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            # ì›ë³¸ í‚¤ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            raw_doc_id = f"{email_data['emailId']}::{i}"
            doc_id = self._sanitize_document_key(raw_doc_id)

            logging.info(f"ë¬¸ì„œ í‚¤ ë³€í™˜: '{raw_doc_id}' â†’ '{doc_id}'")

            document = {
                "@search.action": "mergeOrUpload",
                "id": doc_id,
                "emailId": email_data["emailId"],
                "conversationId": email_data["conversationId"],
                "subject": email_data["subject"],
                "from_name": email_data["from"]["name"],
                "from_email": email_data["from"]["email"],
                "to_names": [p["name"] for p in email_data["to"]],
                "cc_names": [p["name"] for p in email_data["cc"]],
                "receivedAt": email_data["receivedAt"],
                "chunk": chunk,
                "bodyPreview": (
                    email_data["body"][:200] + "..."
                    if len(email_data["body"]) > 200
                    else email_data["body"]
                ),
                "chunkEmbedding": embedding,
                "webLink": "",
                "html_body": email_data.get("html_body", ""),
            }

            # ì•¡ì…˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if action_data:
                # action_data['due']ê°€ ISO(UTC)ì¼ ìˆ˜ë„(Noneì¼ ìˆ˜ë„) ìˆìœ¼ë‹ˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                document.update(
                    {
                        "action_type": action_data.get("type", ""),
                        "assignee": action_data.get("assignee", ""),
                        "due": action_data.get("due"),  # ë” ì´ìƒ T00:00:00Z ë¶™ì´ì§€ ì•ŠìŒ
                        "priority": action_data.get("priority", ""),
                        "tags": action_data.get("tags", []),
                        "confidence": action_data.get("confidence", 0.0),
                    }
                )

            documents.append(document)

        # ë°°ì¹˜ ì—…ë¡œë“œ
        try:
            result = self.search_client.upload_documents(documents)
            logging.info(f"âœ… Search ì¸ë±ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ")
            return result

        except Exception as e:
            logging.error(f"âŒ Search ì¸ë±ìŠ¤ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def save_to_table_storage(self, action_data: Dict, email_data: Dict) -> None:
        """Actions í…Œì´ë¸”ì— ì €ì¥"""

        if not action_data:
            return

        try:
            actions_table = self.table_service.get_table_client("Actions")

            # RowKeyë„ ì •ì œ
            raw_row_key = f"{email_data['emailId']}::0"
            row_key = self._sanitize_document_key(raw_row_key)

            entity = {
                "PartitionKey": "techcorp",
                "RowKey": row_key,
                "subject": email_data["subject"],
                "title": action_data.get("title", ""),
                "assignee": action_data.get("assignee", ""),
                "due": action_data.get("due", ""),
                "priority": action_data.get("priority", ""),
                "type": action_data.get("type", ""),
                "tags": ";".join(action_data.get("tags", [])),
                "confidence": action_data.get("confidence", 0.0),
                "receivedAt": email_data["receivedAt"],
                "conversationId": email_data.get("conversationId", ""),
                "webLink": "",
            }

            actions_table.upsert_entity(entity)
            logging.info(f"âœ… Actions í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {action_data['title']}")

        except Exception as e:
            logging.error(f"âŒ Actions í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {e}")

    def process_emails(self, email_file_path: str) -> Dict:
        """ì´ë©”ì¼ ë°°ì¹˜ ì²˜ë¦¬"""

        logging.info(f"ğŸš€ ì´ë©”ì¼ ì²˜ë¦¬ ì‹œì‘: {email_file_path}")

        # ì´ë©”ì¼ ë°ì´í„° ë¡œë“œ
        emails = self.load_email_data(email_file_path)

        # ì²˜ë¦¬ í†µê³„
        stats = {
            "total_emails": len(emails),
            "processed_emails": 0,
            "actions_extracted": 0,
            "errors": [],
        }

        # ê° ì´ë©”ì¼ ì²˜ë¦¬
        for item in emails:
            try:
                # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
                if not isinstance(item, dict):
                    logging.warning(f"âš ï¸ ì˜ëª»ëœ ì•„ì´í…œ í˜•ì‹ ê±´ë„ˆëœ€: {type(item)}")
                    continue

                email_data = item.get("data")
                record_id = item.get("recordId", "unknown")

                # data í•„ë“œ ê²€ì¦
                if not email_data:
                    logging.warning(f"âš ï¸ data í•„ë“œê°€ ì—†ëŠ” ë ˆì½”ë“œ ê±´ë„ˆëœ€: {record_id}")
                    continue

                if not isinstance(email_data, dict):
                    logging.warning(
                        f"âš ï¸ data í•„ë“œê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ë ˆì½”ë“œ ê±´ë„ˆëœ€: {record_id}"
                    )
                    continue

                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                required_fields = ["subject", "email_body", "from_address"]
                missing_fields = [
                    field for field in required_fields if not email_data.get(field)
                ]

                if missing_fields:
                    logging.warning(
                        f"âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ê±´ë„ˆëœ€ {record_id}: {missing_fields}"
                    )
                    continue

                # 1. ì „ì²˜ë¦¬
                standardized_email = self.preprocess_email(email_data)
                logging.info(f"ğŸ“§ ì²˜ë¦¬ ì¤‘: {standardized_email['subject']}")

                # 2. ê° ìˆ˜ì‹ ìë³„ë¡œ ê°œì¸í™”ëœ ë¶„ì„ (ìƒ˜í”Œë¡œ ë°•ì§€í›ˆ ê¸°ì¤€)
                user_context = {
                    "name": "ë°•ì§€í›ˆ",
                    "email": "jihoon.park@techcorp.com",
                    "team": "ë°±ì—”ë“œê°œë°œíŒ€",
                }

                # 3. ì •ì±… ì—”ì§„ ì ìš©
                policy_signals = self.analyze_with_policy_engine(
                    email_data, user_context
                )
                logging.info(f"ğŸ“‹ ì •ì±… ë¶„ì„: {policy_signals['policy_decision']}")

                # 4. LLM ì•¡ì…˜ ì¶”ì¶œ
                action_result = self.extract_actions_with_llm(
                    standardized_email, policy_signals, user_context
                )

                # 5. ì•¡ì…˜ ì •ê·œí™”
                normalized_action = None
                if action_result.get("is_action"):
                    normalized_action = self.normalize_action(
                        action_result, standardized_email
                    )
                    if normalized_action:
                        stats["actions_extracted"] += 1
                        logging.info(f"âš¡ ìµœì¢… ë³´ì • ì™„ë£Œ: {normalized_action}")

                # 6. Azure AI Search ì—…ë¡œë“œ
                self.upload_to_search(standardized_email, normalized_action)

                # 7. Actions í…Œì´ë¸” ì €ì¥
                if normalized_action:
                    self.save_to_table_storage(normalized_action, standardized_email)

                stats["processed_emails"] += 1

            except Exception as e:
                error_msg = f"ì´ë©”ì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {record_id} - {e}"
                logging.error(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)

        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        logging.info("ğŸ‰ ì´ë©”ì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        logging.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        logging.info(f"   - ì´ ì´ë©”ì¼: {stats['total_emails']}ê°œ")
        logging.info(f"   - ì²˜ë¦¬ ì„±ê³µ: {stats['processed_emails']}ê°œ")
        logging.info(f"   - ì•¡ì…˜ ì¶”ì¶œ: {stats['actions_extracted']}ê°œ")
        logging.info(f"   - ì˜¤ë¥˜: {len(stats['errors'])}ê°œ")

        return stats


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    try:
        # ì´ë©”ì¼ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        processor = EmailProcessor()

        # ì´ë©”ì¼ íŒŒì¼ ê²½ë¡œ
        email_file = "../data/email_sample.json"

        if not os.path.exists(email_file):
            logging.error(f"âŒ ì´ë©”ì¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {email_file}")
            return

        # ì´ë©”ì¼ ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_emails(email_file)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 50)
        print("ğŸ“Š ìµœì¢… ì²˜ë¦¬ ê²°ê³¼")
        print("=" * 50)
        print(f"ì´ ì´ë©”ì¼: {results['total_emails']}ê°œ")
        print(f"ì²˜ë¦¬ ì„±ê³µ: {results['processed_emails']}ê°œ")
        print(f"ì•¡ì…˜ ì¶”ì¶œ: {results['actions_extracted']}ê°œ")
        print(f"ì˜¤ë¥˜: {len(results['errors'])}ê°œ")

        if results["errors"]:
            print("\nâŒ ì˜¤ë¥˜ ëª©ë¡:")
            for error in results["errors"]:
                print(f"  - {error}")

        print("\nâœ… ì²˜ë¦¬ ì™„ë£Œ! Azure AI Searchì™€ Table Storageì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    except Exception as e:
        logging.error(f"âŒ ë©”ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    main()
